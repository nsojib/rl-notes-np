{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f307c197-0e88-4a3c-a39e-192c8d3c5178",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import gym \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099f8265-bd58-4fce-a6b6-702d10b78fbd",
   "metadata": {},
   "source": [
    "### actor critic worked. TODO: gradient check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc73e3ed-1292-4777-b5eb-88079d796b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name='CartPole-v1'\n",
    "# env_name='CartPole-v0'\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b496b19-35aa-4f32-9c96-5ac5642ad616",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40147468-e59d-405c-b4aa-dedd2f1abdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoNN():\n",
    "    #numpy model\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        print('simple two layer neural network')\n",
    "        print(f'creating nn: #input:{n_feature} #hidden:{n_hidden} #output:{n_output}')\n",
    "        self.n_output=n_output\n",
    "        self.model={}\n",
    "        self.model['w1']=np.random.randn(n_hidden, n_feature)/ np.sqrt(n_hidden) # \"Xavier\" initialization\n",
    "        self.model['b1']= np.zeros((1,n_hidden))\n",
    "        self.model['w2']=np.random.randn(n_output, n_hidden)/ np.sqrt(n_output) # \"Xavier\" initialization\n",
    "        self.model['b2'] = np.zeros((1,n_output))\n",
    " \n",
    "    def softmax(self, Z): \n",
    "        expz = np.exp(Z - np.max(Z))                    #to prevent overflow\n",
    "        return expz / expz.sum(axis=1, keepdims=True)   #reduce columns for each row.\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X: Nxn_feature\n",
    "        \"\"\"\n",
    "        self.X=X\n",
    "        self.h=X  @ self.model['w1'].T  + self.model['b1']             #(NxD)@(DxH)  \n",
    "        self.h[self.h<0]=0                           #relu \n",
    "        z=self.h @ self.model['w2'].T + self.model['b2']\n",
    "        if self.n_output==1: #regression.\n",
    "            self.out=z\n",
    "            return self.out\n",
    "        \n",
    "        self.out=self.softmax(z)    #(NxH)@(HxO)\n",
    "        return self.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "246083ef-a300-4146-a6e5-dc5325f5dcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAdam:\n",
    "    def __init__(self, weights, learning_rate=0.001, beta1 = .9, beta2 = .999):\n",
    "        \"\"\"\n",
    "        weights: key,value pair of weights.\n",
    "        \"\"\"\n",
    "        self.weights=weights\n",
    "        self.learning_rate=learning_rate\n",
    "        self.beta1=beta1\n",
    "        self.beta2=beta2\n",
    "        self.M = {k: np.zeros_like(v) for k, v in weights.items()}\n",
    "        self.V = {k: np.zeros_like(v) for k, v in weights.items()}\n",
    "        self.eps = 1e-8  # Smoothing to avoid division by zero\n",
    "        self.t=0\n",
    "        \n",
    "    def update(self, grads):\n",
    "        self.t +=1\n",
    "        for k in grads:\n",
    "            self.M[k] = self.beta1 * self.M[k] + (1. - self.beta1) * grads[k]\n",
    "            self.V[k] = self.beta2 * self.V[k] + (1. - self.beta2) * grads[k]**2\n",
    "\n",
    "            m_k_hat = self.M[k] / (1. - self.beta1**self.t)  #bias correction\n",
    "            v_k_hat = self.V[k] / (1. - self.beta2**self.t)  #bias correction\n",
    "            self.weights[k] -= self.learning_rate * m_k_hat / (np.sqrt(v_k_hat) + self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39504d8-edfe-4c74-96fe-7c117dcfc99b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6e6684b-1796-4ffa-aa58-9835a50511e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple two layer neural network\n",
      "creating nn: #input:4 #hidden:128 #output:2\n",
      "simple two layer neural network\n",
      "creating nn: #input:4 #hidden:128 #output:1\n"
     ]
    }
   ],
   "source": [
    "actor_np=TwoNN(state_dim, 128, n_actions) \n",
    "critic_np=TwoNN(state_dim, 128, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b14e1a71-74d0-4f08-ab2f-82404bceb2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=1e-3 \n",
    "gamma = 0.99\n",
    "\n",
    "learning_rate=1e-5 \n",
    "learning_rate = 0.001\n",
    "nepisode=500\n",
    "lr=learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e898769-defc-41de-ac54-af5350de8e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_ac=MyAdam(weights=actor_np.model, learning_rate=learning_rate)\n",
    "adam_cr=MyAdam(weights=critic_np.model, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c5e3c02-fc02-49a9-b150-fabbc7355b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f0742d9-8d2c-4b35-ac91-0e1698881b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grads_manual_np(tP, y_train, th, X, W2, adv, N=1):\n",
    "    # print('shape:', tP.shape, y_train.shape)\n",
    "    grads={}\n",
    "    \n",
    "    dz2=(tP-y_train)*adv  /N \n",
    "    dw2=(dz2.T  @  th) \n",
    "    dh=(dz2 @   W2  )  \n",
    "    db2=np.sum(dz2, axis=0, keepdims=True)  \n",
    " \n",
    "    dh[th<=0]=0                     #equal sign is extremely important. \n",
    " \n",
    "    dw1 = dh.T @ X.reshape(1,-1)\n",
    "    db1 = np.sum(dh, axis=0, keepdims=True) \n",
    "    grads={'w1':dw1, 'b1':db1, 'w2':dw2, 'b2':db2}\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b048199-8579-46cd-9531-3fad9d5caf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grads_manual_critic_np(yhat, y, th, th_next, state, next_state, w2, done, N=1):\n",
    "    grads={}\n",
    "    \n",
    "    dz2=(yhat - y)  /N \n",
    "    cth=(1-done)*gamma*th_next-th\n",
    "    dw2=(dz2.T  @  cth) \n",
    "\n",
    "    dh=(dz2 @   w2  ) \n",
    " \n",
    "    db2=np.sum(dz2*((1-done)*gamma-1), axis=0, keepdims=True)  \n",
    "     \n",
    "    dz1n=dh.copy()\n",
    "    dz1n[th_next<=0]=0                     #equal sign is extremely important. \n",
    "    \n",
    "     \n",
    "    dz1=dh.copy()\n",
    "    dz1[th<=0]=0                     #equal sign is extremely important. \n",
    "\n",
    "     \n",
    "    dw1=(1-done)* gamma* dz1n.T @  next_state.reshape(1,-1) - dz1.T @ state.reshape(1,-1)  \n",
    "    \n",
    "    \n",
    "    db1=np.sum( ((1-done)*gamma*dz1n -dz1), axis=0 , keepdims=True)\n",
    "    \n",
    "    grads={'w1':dw1, 'b1':db1, 'w2':dw2, 'b2':db2}\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d067df08-bf74-4117-b918-71c12c5e7628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:0 score:25.0 last_20:25.0\n",
      "episode:20 score:75.0 last_20:36.5\n",
      "episode:40 score:37.0 last_20:51.25\n",
      "episode:60 score:19.0 last_20:59.45\n",
      "episode:80 score:24.0 last_20:28.8\n",
      "episode:100 score:45.0 last_20:79.9\n",
      "episode:120 score:14.0 last_20:94.5\n",
      "episode:140 score:307.0 last_20:151.4\n",
      "episode:160 score:500.0 last_20:389.0\n",
      "episode:180 score:487.0 last_20:396.45\n",
      "episode:200 score:14.0 last_20:452.2\n",
      "episode:220 score:500.0 last_20:257.75\n",
      "episode:240 score:134.0 last_20:415.0\n",
      "episode:260 score:500.0 last_20:460.0\n",
      "episode:280 score:218.0 last_20:393.8\n",
      "episode:300 score:17.0 last_20:96.3\n",
      "episode:320 score:500.0 last_20:144.75\n",
      "episode:340 score:220.0 last_20:219.7\n",
      "episode:360 score:271.0 last_20:234.85\n",
      "episode:380 score:120.0 last_20:164.5\n",
      "episode:400 score:103.0 last_20:106.55\n",
      "episode:420 score:105.0 last_20:65.05\n",
      "episode:440 score:479.0 last_20:354.05\n",
      "episode:460 score:500.0 last_20:500.0\n",
      "-------solved in 460 steps-------\n"
     ]
    }
   ],
   "source": [
    "episode_rewards = []\n",
    " \n",
    "for i in range(nepisode+1):\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "\n",
    "\n",
    "    while not done: \n",
    "        probs_np=actor_np.forward(state)\n",
    "        ath_np=actor_np.h.copy()\n",
    "        a= np.random.choice([0,1],p=probs_np[0])   \n",
    "        next_state, reward, done, info = env.step(a)\n",
    "        total_reward += reward   \n",
    "        \n",
    "        y_next=critic_np.forward(next_state.reshape(1,-1) )\n",
    "        th_next=critic_np.h.copy() \n",
    "        y=critic_np.forward(state.reshape(1,-1))\n",
    "        th=critic_np.h.copy() \n",
    "        yhat=reward + (1-done)*gamma*y_next\n",
    "        \n",
    "        \n",
    "        grads_critic_np=grads_manual_critic_np(yhat, y, th, th_next, state, next_state, critic_np.model['w2'], done, N=1)\n",
    "        adam_cr.update(grads_critic_np)\n",
    "        # for k,v in grads_critic_np.items():\n",
    "        #     critic_np.model[k] -=lr*grads_critic_np[k]\n",
    "        \n",
    "        advantage = yhat - y\n",
    "        yt=np.eye(2)[a].reshape(1,-1)  \n",
    "        grads_actor_np=grads_manual_np(probs_np, yt, ath_np, state, actor_np.model['w2'], advantage , N=1)\n",
    "\n",
    "        # for k,v in grads_actor_np.items():\n",
    "        #     actor_np.model[k] -=lr*grads_actor_np[k]\n",
    "        adam_ac.update(grads_actor_np)\n",
    "        \n",
    "        \n",
    "        state = next_state\n",
    "            \n",
    "    episode_rewards.append(total_reward)\n",
    "    if i%20==0:\n",
    "        avg20=np.mean(episode_rewards[-20:])\n",
    "        print(f'episode:{i} score:{total_reward} last_20:{avg20}')\n",
    "        if avg20>=env.spec.reward_threshold:\n",
    "            print(f'-------solved in {i} steps-------')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9abae8c-ab08-4214-a719-6cd05431a9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_np(env, actor, num_ep=20): \n",
    "    \"\"\"\n",
    "    run env num_ep times and return average reward.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for ep in range(num_ep):\n",
    "        state = env.reset() \n",
    "        ep_reward=0\n",
    "        for tt in range(2000):\n",
    "            probs = actor.forward(state.reshape(1,-1))  \n",
    "            action=np.argmax(probs[0])\n",
    "\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            ep_reward+=reward\n",
    "            if done:\n",
    "                # print(f\"ep:{ep} reward:{ep_reward}\")\n",
    "                break \n",
    "        \n",
    "        rewards.append(ep_reward)\n",
    "    return np.array(rewards).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8e57c2a-6d40-41f9-83ab-121df8c83846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_r 500.0\n"
     ]
    }
   ],
   "source": [
    "avg_r=evaluate_model_np(env, actor_np)\n",
    "print('avg_r',avg_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7baa7b60-c349-4178-9486-51c1e4f9e39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------solved in 220 steps------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930c8244-0e3d-4eeb-9d1f-86e2c065361c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ac54536-6431-4064-a1cc-422aa8eb60d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "state = env.reset() \n",
    "ep_reward=0\n",
    "for tt in range(2000):\n",
    "    env.render()\n",
    "    probs = actor_np.forward(state.reshape(1,-1))  \n",
    "    action=np.argmax(probs[0]) \n",
    "    state, reward, done, _ = env.step(action)\n",
    "    ep_reward+=reward\n",
    "    if done:\n",
    "        break \n",
    "env.close()\n",
    "print('total reward:', ep_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e723a2-196d-481c-85bb-fa8d9047a5e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5822a4e8-62ce-4a3a-8bd0-e494076f877a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d813839b-dfd2-4ed5-b088-ae9f04b2260f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2e10aa-6e05-4b3e-b0e0-de8bb16b74cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
