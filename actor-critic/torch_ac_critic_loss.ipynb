{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3648b7f-1c38-492e-8c6e-896943e60abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c07170-b361-4592-a880-50313747d09f",
   "metadata": {},
   "source": [
    "## challange solved in 3 weeks (torch only, np yet ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac096c67-23d8-43fe-b5ab-c601b71d2b82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "741a815f-cdce-446b-b3df-2d3b0e6530eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name='CartPole-v1'\n",
    "env_name='CartPole-v0'\n",
    "env = gym.make(env_name)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59cf465e-0187-478c-88a2-ea9a50873842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input features:  4  output class:  2\n"
     ]
    }
   ],
   "source": [
    "seed=4\n",
    "np.random.seed(seed)\n",
    "env.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "N,D=1, 4\n",
    "H= 128     \n",
    "C=2 \n",
    "\n",
    "sw1 = 0.04*torch.randn(H , D,  dtype=torch.float) \n",
    "sb1 = torch.zeros((1,H))\n",
    "sw2 = 0.04*torch.randn(C, H, dtype=torch.float) \n",
    "sb2 = torch.zeros((1,C))\n",
    "\n",
    "sw1c = torch.randn(H , D,  dtype=torch.float)   #for critic\n",
    "sb1c = torch.zeros((1,H))                           #for critic\n",
    "sw2c = torch.randn(1, H, dtype=torch.float)    #for critic\n",
    "sb2c = torch.zeros((1,1))                           #for critic\n",
    "\n",
    "\n",
    "print('input features: ',D ,' output class: ',C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "275aba82-9fbb-4ea1-8fed-d7a640c8e0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def t(x): return torch.from_numpy(x).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5fc26a3-39f0-45b8-be9a-79c38e565336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tn(tnsr): return tnsr.detach().numpy().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2be5b8cf-17bf-4382-a33a-f34d139d89e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    #same performance with torch.optim.Adam\n",
    "    def __init__(self, model_params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8):\n",
    "        self.params = list(model_params)\n",
    "        self.lr = lr\n",
    "        self.beta_1, self.beta_2 = betas\n",
    "        self.eps = eps\n",
    "        self.M= [torch.zeros_like(p) for p in self.params]\n",
    "        self.V = [torch.zeros_like(p) for p in self.params]\n",
    "        self.n_steps = 0\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            param.grad = None\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        self.n_steps += 1 \n",
    "        for i in range(len(self.params)): \n",
    "            dw=self.params[i].grad\n",
    "             \n",
    "            self.M[i]= self.M[i]*self.beta_1 + (1-self.beta_1) * dw\n",
    "            self.V[i] = self.V[i] *self.beta_2 + (1 - self.beta_2)* dw**2\n",
    " \n",
    "            m_hat = self.M[i] / (1 - self.beta_1 ** self.n_steps)  #bias correction\n",
    "            v_hat = self.V[i] / (1 - self.beta_2 ** self.n_steps) #bias correction\n",
    "             \n",
    "            self.params[i] -= self.lr * m_hat / (torch.sqrt(v_hat)+self.eps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdbc84b5-bf6c-48d1-bd63-79aa17b6712d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_Open_Torch:\n",
    "    def __init__(self, D, H, C):\n",
    "        self.tW1 = torch.randn(H , D,  dtype=torch.float, requires_grad=True)\n",
    "        self.tb1 = torch.zeros((1,H), requires_grad=True)\n",
    "        self.tW2 = torch.randn(C,H, dtype=torch.float, requires_grad=True)\n",
    "        self.tb2 = torch.zeros((1,C), requires_grad=True)\n",
    "        self.C=C\n",
    "        self.params=[self.tW1, self.tb1, self.tW2, self.tb2]\n",
    "        self.ws={'w1':self.tW1, 'b1':self.tb1, 'w2':self.tW2, 'b2':self.tb2}\n",
    "        \n",
    "    def parameters(self):\n",
    "        return self.params \n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            param.grad=None\n",
    "        \n",
    "    def forward(self, X):\n",
    "        th= torch.relu( X   @ self.tW1.T  + self.tb1) \n",
    "        z=th @self.tW2.T  + self.tb2 \n",
    "        if self.C==1:\n",
    "            return th, z\n",
    "        \n",
    "        tP=torch.softmax( z , axis=1)\n",
    "        return th, tP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa98fa3b-f968-46a2-a7ed-d685c6d9fabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grads_auto(model):\n",
    "    grads={}\n",
    "    with torch.no_grad():\n",
    "        grads['w1']=model.tW1.grad.clone()\n",
    "        grads['b1']=model.tb1.grad.clone()\n",
    "        grads['w2']=model.tW2.grad.clone()\n",
    "        grads['b2']=model.tb2.grad.clone()\n",
    "        \n",
    "        model.tb1.grad.zero_() \n",
    "        model.tW1.grad.zero_() \n",
    "        model.tb2.grad.zero_() \n",
    "        model.tW2.grad.zero_() \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9fccd14-4304-41f5-b440-b5d877ca8a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(env, num_ep=20):\n",
    "    np.random.seed(seed)\n",
    "    env.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    \"\"\"\n",
    "    run env num_ep times and return average reward.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for ep in range(num_ep):\n",
    "        state = env.reset() \n",
    "        ep_reward=0\n",
    "        for tt in range(2000):\n",
    "            h,probs = actor.forward(t(state))\n",
    "            action=torch.argmax(probs)\n",
    "            # dist = torch.distributions.Categorical(probs=probs[0])\n",
    "            # action = dist.sample()\n",
    "            # action=action.detach().numpy() \n",
    "\n",
    "            state, reward, done, _ = env.step(action.numpy())\n",
    "            ep_reward+=reward\n",
    "            if done:\n",
    "                # print(f\"ep:{ep} reward:{ep_reward}\")\n",
    "                break \n",
    "        \n",
    "        rewards.append(ep_reward)\n",
    "    return np.array(rewards).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7383cd2-1f58-448d-84dd-37e6dbf4c66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "nepisode=200\n",
    "nepisode=500\n",
    "lr=0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98017a4-3488-4f32-985f-3f2c55334111",
   "metadata": {},
   "source": [
    "### run1: base 59.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa50291a-7b13-4f0b-ad65-a84cf46d1027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f94e65cb490>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "env.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "857d0a4b-e635-4a11-8dc8-a029dda89e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor=NN_Open_Torch(state_dim, 128, n_actions)\n",
    "critic=NN_Open_Torch(state_dim, 128, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac97685f-4e13-49fd-8216-015331e8a645",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    actor.tW1.data=sw1.clone()\n",
    "    actor.tb1.data=sb1.clone()\n",
    "    actor.tW2.data=sw2.clone()\n",
    "    actor.tb2.data=sb2.clone() \n",
    "    \n",
    "    critic.tW1.data=sw1c.clone()\n",
    "    critic.tb1.data=sb1c.clone()\n",
    "    critic.tW2.data=sw2c.clone()\n",
    "    critic.tb2.data=sb2c.clone() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52595bdf-eb6b-41de-9497-dc0670ba523e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:0 score:15.0 last_20:15.0\n",
      "episode:20 score:15.0 last_20:14.45\n",
      "episode:40 score:15.0 last_20:19.1\n",
      "episode:60 score:62.0 last_20:32.25\n",
      "episode:80 score:24.0 last_20:41.75\n",
      "episode:100 score:14.0 last_20:48.6\n",
      "episode:120 score:21.0 last_20:62.35\n",
      "episode:140 score:19.0 last_20:21.45\n",
      "episode:160 score:29.0 last_20:23.7\n",
      "episode:180 score:44.0 last_20:35.1\n",
      "episode:200 score:50.0 last_20:57.2\n",
      "episode:220 score:58.0 last_20:70.95\n",
      "episode:240 score:75.0 last_20:73.45\n",
      "episode:260 score:46.0 last_20:56.2\n",
      "episode:280 score:51.0 last_20:44.45\n",
      "episode:300 score:57.0 last_20:43.2\n",
      "episode:320 score:52.0 last_20:44.7\n",
      "episode:340 score:67.0 last_20:72.2\n",
      "episode:360 score:58.0 last_20:73.2\n",
      "episode:380 score:126.0 last_20:102.0\n",
      "episode:400 score:90.0 last_20:57.75\n",
      "episode:420 score:93.0 last_20:61.15\n",
      "episode:440 score:136.0 last_20:77.65\n",
      "episode:460 score:51.0 last_20:61.0\n",
      "episode:480 score:79.0 last_20:71.35\n",
      "episode:500 score:97.0 last_20:96.8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "episode_rewards = []\n",
    "\n",
    "for i in range(nepisode+1):\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "\n",
    "\n",
    "    while not done:\n",
    "        x=t(state ) \n",
    "        th, probs=actor.forward(x) \n",
    "        dist = torch.distributions.Categorical(probs=probs[0])\n",
    "        action = dist.sample() \n",
    "        a=action.detach().data.numpy()  \n",
    "        \n",
    "        \n",
    "        next_state, reward, done, info = env.step(a)\n",
    "        advantage = reward + (1-done)*gamma*critic.forward(t(next_state))[1] - critic.forward(t(state))[1]\n",
    "        \n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "\n",
    "        critic_loss = 0.5*advantage.pow(2).mean() \n",
    "        critic.zero_grad()\n",
    "        critic_loss.backward() \n",
    "        # adam_critic.step() \n",
    "        with torch.no_grad():\n",
    "            for param in critic.parameters():\n",
    "                param.data -=lr*param.grad \n",
    "        \n",
    "        \n",
    "         \n",
    "        actor_loss = -dist.log_prob(action)*advantage.detach()\n",
    "        actor.zero_grad() \n",
    "        actor_loss.backward()\n",
    "        # adam_actor.step() \n",
    "        \n",
    "        with torch.no_grad(): \n",
    "            for param in actor.parameters():\n",
    "                param.data -=lr*param.grad\n",
    "            \n",
    "    episode_rewards.append(total_reward)\n",
    "    if i%20==0:\n",
    "        avg20=np.mean(episode_rewards[-20:])\n",
    "        print(f'episode:{i} score:{total_reward} last_20:{avg20}')\n",
    "        if avg20>=env.spec.reward_threshold:\n",
    "            print(f'-------solved in {i} steps-------')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27d20faf-3559-44b9-be60-895d91876ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_r 79.95\n"
     ]
    }
   ],
   "source": [
    "avg_r=evaluate_model(env)\n",
    "print('avg_r',avg_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95beb130-ef2b-45c7-bca6-74759230594a",
   "metadata": {},
   "source": [
    "### end of run1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4234b410-41f0-4a68-956f-c37d67bdca35",
   "metadata": {},
   "source": [
    "### now run2: using auto_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2795176a-0b83-45bd-ab9b-8548694b72ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "env.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "actor=NN_Open_Torch(state_dim, 128, n_actions)\n",
    "critic=NN_Open_Torch(state_dim, 128, 1) \n",
    "\n",
    "with torch.no_grad():\n",
    "    actor.tW1.data=sw1.clone()\n",
    "    actor.tb1.data=sb1.clone()\n",
    "    actor.tW2.data=sw2.clone()\n",
    "    actor.tb2.data=sb2.clone() \n",
    "    \n",
    "    critic.tW1.data=sw1c.clone()\n",
    "    critic.tb1.data=sb1c.clone()\n",
    "    critic.tW2.data=sw2c.clone()\n",
    "    critic.tb2.data=sb2c.clone() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f96ab3d-c125-4e8c-b358-bad71b370dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9c862b5-7518-447f-b059-6e5b811c904e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:0 score:15.0 last_20:15.0\n",
      "episode:20 score:15.0 last_20:14.45\n",
      "episode:40 score:15.0 last_20:19.1\n",
      "episode:60 score:62.0 last_20:32.25\n",
      "episode:80 score:24.0 last_20:41.75\n",
      "episode:100 score:14.0 last_20:48.6\n",
      "episode:120 score:21.0 last_20:62.35\n",
      "episode:140 score:19.0 last_20:21.45\n",
      "episode:160 score:29.0 last_20:23.7\n",
      "episode:180 score:44.0 last_20:35.1\n",
      "episode:200 score:50.0 last_20:57.2\n",
      "episode:220 score:58.0 last_20:70.95\n",
      "episode:240 score:75.0 last_20:73.45\n",
      "episode:260 score:46.0 last_20:56.2\n",
      "episode:280 score:51.0 last_20:44.45\n",
      "episode:300 score:57.0 last_20:43.2\n",
      "episode:320 score:52.0 last_20:44.7\n",
      "episode:340 score:67.0 last_20:72.2\n",
      "episode:360 score:58.0 last_20:73.2\n",
      "episode:380 score:126.0 last_20:102.0\n",
      "episode:400 score:90.0 last_20:57.75\n",
      "episode:420 score:93.0 last_20:61.15\n",
      "episode:440 score:136.0 last_20:77.65\n",
      "episode:460 score:51.0 last_20:61.0\n",
      "episode:480 score:79.0 last_20:71.35\n",
      "episode:500 score:97.0 last_20:96.8\n"
     ]
    }
   ],
   "source": [
    "episode_rewards = []\n",
    "lr=0.01\n",
    "for i in range(nepisode+1):\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "\n",
    "\n",
    "    while not done:\n",
    "        x=t(state ) \n",
    "        th, probs=actor.forward(x) \n",
    "        dist = torch.distributions.Categorical(probs=probs[0])\n",
    "        action = dist.sample()\n",
    "        \n",
    "        a=action.detach().data.numpy()  \n",
    "        next_state, reward, done, info = env.step(a)\n",
    "        yhat=reward + (1-done)*gamma*critic.forward(t(next_state))[1]\n",
    "        y=critic.forward(t(state))[1]\n",
    "        advantage = yhat - y\n",
    "        \n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "\n",
    "        critic_loss = 0.5*advantage.pow(2).mean() \n",
    "        critic.zero_grad()\n",
    "        critic_loss.backward() \n",
    "        # gradsm_critic=grads_manual(yhat,  y, th, state.reshape(1,-1), critic.tW2)   \n",
    "        gradsm_critic=grads_auto(critic)\n",
    "        \n",
    "        with torch.no_grad(): \n",
    "            for k,v in gradsm_critic.items():\n",
    "                critic.ws[k] -=lr*gradsm_critic[k]\n",
    " \n",
    "        actor_loss = -dist.log_prob(action)*advantage.detach()\n",
    "        actor.zero_grad() \n",
    "        actor_loss.backward()\n",
    "        gradsm_actor=grads_auto(actor)\n",
    "        \n",
    "        with torch.no_grad(): \n",
    "            for k,v in gradsm_actor.items():\n",
    "                actor.ws[k] -=lr*gradsm_actor[k]\n",
    "                \n",
    "            \n",
    "    episode_rewards.append(total_reward)\n",
    "    if i%20==0:\n",
    "        avg20=np.mean(episode_rewards[-20:])\n",
    "        print(f'episode:{i} score:{total_reward} last_20:{avg20}')\n",
    "        if avg20>=env.spec.reward_threshold:\n",
    "            print(f'-------solved in {i} steps-------')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3f27935-d514-4754-98c6-114617ef87db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_r 79.95\n"
     ]
    }
   ],
   "source": [
    "avg_r=evaluate_model(env)\n",
    "print('avg_r',avg_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afc7864-a296-476f-9842-8c956f72a7d6",
   "metadata": {},
   "source": [
    "### end of run2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d257a01a-714b-41b8-a253-141e37ecfe78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8aded06d-b859-4a1f-91d8-0da492acab76",
   "metadata": {},
   "source": [
    "### Now, run3: actor manual_grad, critic augo_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da611c1d-e295-4bd3-a80c-339cfc86f789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grads_manual(tP, y_train, th, X, W2, adv, N=1):\n",
    "    # print('shape:', tP.shape, y_train.shape)\n",
    "    grads={}\n",
    "    \n",
    "    dz2=(tP-y_train)*adv  /N \n",
    "    dW2=(dz2.T  @  th) \n",
    "    dh=(dz2 @   W2  ) \n",
    "    db2=torch.sum(dz2 , axis=0 ) \n",
    "\n",
    "\n",
    "    dz1=torch.tensor(dh)   \n",
    "    dz1[th<=0]=0                     #equal sign is extremely important. \n",
    "\n",
    "    dW1=( dz1.T @ X) \n",
    "    db1=torch.sum(dz1, axis=0 ) \n",
    "    \n",
    "    grads={'w1':dW1, 'b1':db1, 'w2':dW2, 'b2':db2}\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff88c9f2-fa05-4c27-965b-1ed0511c0bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "env.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "actor=NN_Open_Torch(state_dim, 128, n_actions)\n",
    "critic=NN_Open_Torch(state_dim, 128, 1) \n",
    "\n",
    "with torch.no_grad():\n",
    "    actor.tW1.data=sw1.clone()\n",
    "    actor.tb1.data=sb1.clone()\n",
    "    actor.tW2.data=sw2.clone()\n",
    "    actor.tb2.data=sb2.clone() \n",
    "    \n",
    "    critic.tW1.data=sw1c.clone()\n",
    "    critic.tb1.data=sb1c.clone()\n",
    "    critic.tW2.data=sw2c.clone()\n",
    "    critic.tb2.data=sb2c.clone() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950971dd-ca63-48c1-b509-bb42d18e87b2",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bfd8de1c-7983-4505-8223-c9cbcd4aead4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:0 score:11.0 last_20:11.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_44320/3130668824.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dz1=torch.tensor(dh)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:20 score:23.0 last_20:18.6\n",
      "episode:40 score:8.0 last_20:19.95\n",
      "episode:60 score:9.0 last_20:17.0\n",
      "episode:80 score:16.0 last_20:26.7\n",
      "episode:100 score:16.0 last_20:24.25\n",
      "episode:120 score:12.0 last_20:23.7\n",
      "episode:140 score:18.0 last_20:28.45\n",
      "episode:160 score:49.0 last_20:29.45\n",
      "episode:180 score:60.0 last_20:34.45\n",
      "episode:200 score:39.0 last_20:39.15\n",
      "episode:220 score:66.0 last_20:44.95\n",
      "episode:240 score:21.0 last_20:56.7\n",
      "episode:260 score:36.0 last_20:62.4\n",
      "episode:280 score:49.0 last_20:65.15\n",
      "episode:300 score:45.0 last_20:56.15\n",
      "episode:320 score:57.0 last_20:85.95\n",
      "episode:340 score:33.0 last_20:60.5\n",
      "episode:360 score:62.0 last_20:46.8\n",
      "episode:380 score:37.0 last_20:46.6\n",
      "episode:400 score:39.0 last_20:29.4\n",
      "episode:420 score:119.0 last_20:50.75\n",
      "episode:440 score:70.0 last_20:70.2\n",
      "episode:460 score:76.0 last_20:77.65\n",
      "episode:480 score:54.0 last_20:61.5\n",
      "episode:500 score:55.0 last_20:64.9\n"
     ]
    }
   ],
   "source": [
    "episode_rewards = []\n",
    "lr=0.01\n",
    "for i in range(nepisode+1):\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "\n",
    "\n",
    "    while not done:\n",
    "        x=t(state ) \n",
    "        ath, probs=actor.forward(x) \n",
    "        # dist = torch.distributions.Categorical(probs=probs[0]) \n",
    "        # a=dist.sample().numpy()\n",
    "        probs_np=probs.detach().numpy().copy()\n",
    "        a= np.random.choice([0,1],p=probs_np[0])  #why torch.dist.. gives better result?\n",
    "        \n",
    "        next_state, reward, done, info = env.step(a)\n",
    "        yhat=reward + (1-done)*gamma*critic.forward(t(next_state))[1]\n",
    "        y=critic.forward(t(state))[1]\n",
    "        advantage = yhat - y\n",
    "        \n",
    "        total_reward += reward \n",
    "\n",
    "        critic_loss = 0.5*advantage.pow(2).mean() \n",
    "        critic.zero_grad()\n",
    "        critic_loss.backward()    \n",
    "        gradsm_critic=grads_auto(critic)\n",
    "        \n",
    "        with torch.no_grad(): \n",
    "            for k,v in gradsm_critic.items():\n",
    "                critic.ws[k] -=lr*gradsm_critic[k]\n",
    " \n",
    "        yt=np.eye(2)[a]\n",
    "        yt=yt.reshape(1,-1)\n",
    "        gradsm_actor=grads_manual(probs.detach(),  t(yt),  ath, state.reshape(1,-1), actor.tW2, advantage)   \n",
    "        \n",
    "        with torch.no_grad(): \n",
    "            for k,v in gradsm_actor.items():\n",
    "                actor.ws[k] -=lr*gradsm_actor[k]\n",
    "        \n",
    "        state = next_state\n",
    "            \n",
    "    episode_rewards.append(total_reward)\n",
    "    if i%20==0:\n",
    "        avg20=np.mean(episode_rewards[-20:])\n",
    "        print(f'episode:{i} score:{total_reward} last_20:{avg20}')\n",
    "        if avg20>=env.spec.reward_threshold:\n",
    "            print(f'-------solved in {i} steps-------')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d18114c4-ed52-43c9-b7fe-4e096ee5aadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_r 75.35\n"
     ]
    }
   ],
   "source": [
    "avg_r=evaluate_model(env)\n",
    "print('avg_r',avg_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d309f8d-eac4-43c6-a500-39d1a778c956",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73740c02-5c94-48ba-a083-21570a98a692",
   "metadata": {},
   "source": [
    "### end of run3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7fb609-d7ba-46cf-9ac1-2b3bcb812d27",
   "metadata": {},
   "source": [
    "### run 4: actor, critic both manual grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1dc51e-31e3-4fec-aff4-ef797497ce7e",
   "metadata": {},
   "source": [
    "### loss not match after episode 420"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ba7fee5-ad35-498b-8894-28a0f9f2fded",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "env.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "actor=NN_Open_Torch(state_dim, 128, n_actions)\n",
    "critic=NN_Open_Torch(state_dim, 128, 1) \n",
    "\n",
    "with torch.no_grad():\n",
    "    actor.tW1.data=sw1.clone()\n",
    "    actor.tb1.data=sb1.clone()\n",
    "    actor.tW2.data=sw2.clone()\n",
    "    actor.tb2.data=sb2.clone() \n",
    "    \n",
    "    critic.tW1.data=sw1c.clone()\n",
    "    critic.tb1.data=sb1c.clone()\n",
    "    critic.tW2.data=sw2c.clone()\n",
    "    critic.tb2.data=sb2c.clone() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "384714c4-5250-4d28-88bd-e48126a24977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grads_manual_critic(yhat, y, th, th_next, state, next_state, w2, done, N=1):\n",
    "    grads={}\n",
    "    \n",
    "    dz2=(yhat - y)  /N \n",
    "    cth=(1-done)*gamma*th_next-th\n",
    "    dw2=(dz2.T  @  cth) \n",
    "\n",
    "    dh=(dz2 @   w2  ) \n",
    "\n",
    "    db2=torch.sum(dz2*((1-done)*gamma-1) , axis=0 ) \n",
    "\n",
    "    \n",
    "    # dz1n=torch.tensor(dh)\n",
    "    dz1n=dh.detach().clone()\n",
    "    dz1n[th_next<=0]=0                     #equal sign is extremely important. \n",
    "    \n",
    "    \n",
    "    # dz1=torch.tensor(dh) \n",
    "    dz1=dh.detach().clone()\n",
    "    dz1[th<=0]=0                     #equal sign is extremely important. \n",
    "\n",
    "     \n",
    "    dw1=(1-done)* gamma* dz1n.T @ t(next_state.reshape(1,-1)) - dz1.T @ t(state.reshape(1,-1) ) \n",
    "    \n",
    "    \n",
    "    db1=torch.sum( ((1-done)*gamma*dz1n -dz1), axis=0 )\n",
    "    \n",
    "    grads={'w1':dw1, 'b1':db1, 'w2':dw2, 'b2':db2}\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7593314b-57bd-47b4-b39d-fbb0ecdd5d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:0 score:11.0 last_20:11.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_44320/3130668824.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dz1=torch.tensor(dh)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:20 score:23.0 last_20:18.6\n",
      "episode:40 score:8.0 last_20:19.95\n",
      "episode:60 score:9.0 last_20:17.0\n",
      "episode:80 score:16.0 last_20:26.7\n",
      "episode:100 score:16.0 last_20:24.25\n",
      "episode:120 score:12.0 last_20:23.7\n",
      "episode:140 score:18.0 last_20:28.45\n",
      "episode:160 score:49.0 last_20:29.45\n",
      "episode:180 score:60.0 last_20:34.45\n",
      "episode:200 score:39.0 last_20:39.15\n",
      "episode:220 score:66.0 last_20:44.95\n",
      "episode:240 score:21.0 last_20:56.7\n",
      "episode:260 score:36.0 last_20:62.4\n",
      "episode:280 score:49.0 last_20:65.15\n",
      "episode:300 score:45.0 last_20:56.15\n",
      "episode:320 score:57.0 last_20:85.95\n",
      "episode:340 score:33.0 last_20:60.5\n",
      "episode:360 score:62.0 last_20:46.8\n",
      "episode:380 score:37.0 last_20:46.6\n",
      "episode:400 score:39.0 last_20:29.4\n",
      "episode:420 score:119.0 last_20:50.75\n",
      "episode:440 score:70.0 last_20:70.2\n",
      "episode:460 score:76.0 last_20:77.65\n",
      "episode:480 score:54.0 last_20:61.5\n",
      "episode:500 score:55.0 last_20:64.9\n"
     ]
    }
   ],
   "source": [
    "episode_rewards = []\n",
    "lr=0.01\n",
    "for i in range(nepisode+1):\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "\n",
    "\n",
    "    while not done:\n",
    "        x=t(state ) \n",
    "        ath, probs=actor.forward(x) \n",
    "#         dist = torch.distributions.Categorical(probs=probs[0])\n",
    "#         action = dist.sample() \n",
    "#         a=action.detach().data.numpy()  \n",
    "        \n",
    "        probs_np=probs.detach().numpy().copy()\n",
    "        a= np.random.choice([0,1],p=probs_np[0])  #why torch.dist.. gives better result?\n",
    "        \n",
    "        \n",
    "        next_state, reward, done, info = env.step(a)\n",
    "        \n",
    "        th_next, y_next=critic.forward(t(next_state))\n",
    "        yhat=reward + (1-done)*gamma*y_next\n",
    "        th, y=critic.forward(t(state)) \n",
    "        advantage = yhat - y\n",
    "       \n",
    "        total_reward += reward  \n",
    "        gradsm_critic=grads_manual_critic(yhat, y, th, th_next, state, next_state, critic.tW2, done)\n",
    "        \n",
    "        with torch.no_grad(): \n",
    "            for k,v in gradsm_critic.items():\n",
    "                critic.ws[k] -=lr*gradsm_critic[k]\n",
    "  \n",
    "        yt=np.eye(2)[a].reshape(1,-1) \n",
    "        gradsm_actor=grads_manual(probs.detach(),  t(yt),  ath, state.reshape(1,-1), actor.tW2, advantage)   \n",
    "        \n",
    "        with torch.no_grad(): \n",
    "            for k,v in gradsm_actor.items():\n",
    "                actor.ws[k] -=lr*gradsm_actor[k]\n",
    "        \n",
    "        state = next_state\n",
    "            \n",
    "    episode_rewards.append(total_reward)\n",
    "    if i%20==0:\n",
    "        avg20=np.mean(episode_rewards[-20:])\n",
    "        print(f'episode:{i} score:{total_reward} last_20:{avg20}')\n",
    "        if avg20>=env.spec.reward_threshold:\n",
    "            print(f'-------solved in {i} steps-------')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3763964-ca41-47ed-a5b5-935281e40a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_r 75.45\n"
     ]
    }
   ],
   "source": [
    "avg_r=evaluate_model(env)\n",
    "print('avg_r',avg_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c22a3a2-a1ee-4b5c-8c9b-8bcc5f00027e",
   "metadata": {},
   "source": [
    "### numpy recreate forward check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ce0fef1-3192-4695-8663-c02acbaff025",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoNN():\n",
    "    #numpy model\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        print('simple two layer neural network')\n",
    "        print(f'creating nn: #input:{n_feature} #hidden:{n_hidden} #output:{n_output}')\n",
    "        self.n_output=n_output\n",
    "        self.model={}\n",
    "        self.model['w1']=np.random.randn(n_hidden, n_feature)/ np.sqrt(n_hidden) # \"Xavier\" initialization\n",
    "        self.model['b1']= np.zeros((1,n_hidden))\n",
    "        self.model['w2']=np.random.randn(n_output, n_hidden)/ np.sqrt(n_output) # \"Xavier\" initialization\n",
    "        self.model['b2'] = np.zeros((1,n_output))\n",
    " \n",
    "    def softmax(self, Z): \n",
    "        expz = np.exp(Z - np.max(Z))                    #to prevent overflow\n",
    "        return expz / expz.sum(axis=1, keepdims=True)   #reduce columns for each row.\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X: Nxn_feature\n",
    "        \"\"\"\n",
    "        self.X=X\n",
    "        self.h=X  @ self.model['w1'].T  + self.model['b1']             #(NxD)@(DxH)  \n",
    "        self.h[self.h<0]=0                           #relu \n",
    "        z=self.h @ self.model['w2'].T + self.model['b2']\n",
    "        if self.n_output==1: #regression.\n",
    "            self.out=z\n",
    "            return self.out\n",
    "        \n",
    "        self.out=self.softmax(z)    #(NxH)@(HxO)\n",
    "        return self.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "17929c65-1d65-459d-bac2-27d76729f0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_np(env, actor, num_ep=20):\n",
    "    np.random.seed(seed)\n",
    "    env.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    \"\"\"\n",
    "    run env num_ep times and return average reward.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for ep in range(num_ep):\n",
    "        state = env.reset() \n",
    "        ep_reward=0\n",
    "        for tt in range(2000):\n",
    "            probs = actor.forward(state.reshape(1,-1)) \n",
    "            # action = np.random.choice(n_actions,p=probs[0])\n",
    "            action=np.argmax(probs[0])\n",
    "\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            ep_reward+=reward\n",
    "            if done:\n",
    "                # print(f\"ep:{ep} reward:{ep_reward}\")\n",
    "                break \n",
    "        \n",
    "        rewards.append(ep_reward)\n",
    "    return np.array(rewards).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c5ce19-f888-43f6-97a3-d66995480cd2",
   "metadata": {},
   "source": [
    "### success: np actor and np critic. <b>(worked)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5f67749-0d6e-4db5-85d5-060a57ab167a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grads_manual_np(tP, y_train, th, X, W2, adv, N=1):\n",
    "    # print('shape:', tP.shape, y_train.shape)\n",
    "    grads={}\n",
    "    \n",
    "    dz2=(tP-y_train)*adv  /N \n",
    "    dw2=(dz2.T  @  th) \n",
    "    dh=(dz2 @   W2  )  \n",
    "    db2=np.sum(dz2, axis=0, keepdims=True)  \n",
    " \n",
    "    dh[th<=0]=0                     #equal sign is extremely important. \n",
    " \n",
    "    dw1 = dh.T @ X.reshape(1,-1)\n",
    "    db1 = np.sum(dh, axis=0, keepdims=True) \n",
    "    grads={'w1':dw1, 'b1':db1, 'w2':dw2, 'b2':db2}\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a6549e68-6e02-401b-9d07-8bcc6c5162d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grads_manual_critic_np(yhat, y, th, th_next, state, next_state, w2, done, N=1):\n",
    "    grads={}\n",
    "    \n",
    "    dz2=(yhat - y)  /N \n",
    "    cth=(1-done)*gamma*th_next-th\n",
    "    dw2=(dz2.T  @  cth) \n",
    "\n",
    "    dh=(dz2 @   w2  ) \n",
    " \n",
    "    db2=np.sum(dz2*((1-done)*gamma-1), axis=0, keepdims=True)  \n",
    "     \n",
    "    dz1n=dh.copy()\n",
    "    dz1n[th_next<=0]=0                     #equal sign is extremely important. \n",
    "    \n",
    "     \n",
    "    dz1=dh.copy()\n",
    "    dz1[th<=0]=0                     #equal sign is extremely important. \n",
    "\n",
    "     \n",
    "    dw1=(1-done)* gamma* dz1n.T @  next_state.reshape(1,-1) - dz1.T @ state.reshape(1,-1)  \n",
    "    \n",
    "    \n",
    "    db1=np.sum( ((1-done)*gamma*dz1n -dz1), axis=0 , keepdims=True)\n",
    "    \n",
    "    grads={'w1':dw1, 'b1':db1, 'w2':dw2, 'b2':db2}\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d760ab6-13a1-4596-a486-83ab3b5fa193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple two layer neural network\n",
      "creating nn: #input:4 #hidden:128 #output:2\n",
      "simple two layer neural network\n",
      "creating nn: #input:4 #hidden:128 #output:1\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "env.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "actor_np=TwoNN(state_dim, 128, n_actions) \n",
    "critic_np=TwoNN(state_dim, 128, 1) \n",
    "\n",
    "\n",
    "actor_np.model['w1']=sw1.numpy().copy()\n",
    "actor_np.model['b1']=sb1.numpy().copy()\n",
    "actor_np.model['w2']=sw2.numpy().copy()\n",
    "actor_np.model['b2']=sb2.numpy().copy()\n",
    "     \n",
    "    \n",
    "critic_np.model['w1']=sw1c.numpy().copy()\n",
    "critic_np.model['b1']=sb1c.numpy().copy()\n",
    "critic_np.model['w2']=sw2c.numpy().copy()\n",
    "critic_np.model['b2']=sb2c.numpy().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c470e79e-d452-442f-aaf4-8d26613e9e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f94e65cb490>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "env.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d7dfd78-cd4b-47f2-b1a1-45bfa479c83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:0 score:11.0 last_20:11.0\n",
      "episode:20 score:23.0 last_20:18.6\n",
      "episode:40 score:8.0 last_20:19.95\n",
      "episode:60 score:9.0 last_20:17.0\n",
      "episode:80 score:16.0 last_20:26.7\n",
      "episode:100 score:16.0 last_20:24.25\n",
      "episode:120 score:12.0 last_20:23.7\n",
      "episode:140 score:18.0 last_20:28.45\n",
      "episode:160 score:49.0 last_20:29.45\n",
      "episode:180 score:60.0 last_20:34.45\n",
      "episode:200 score:39.0 last_20:39.15\n",
      "episode:220 score:66.0 last_20:44.95\n",
      "episode:240 score:21.0 last_20:56.7\n",
      "episode:260 score:36.0 last_20:62.4\n",
      "episode:280 score:49.0 last_20:65.15\n",
      "episode:300 score:45.0 last_20:56.15\n",
      "episode:320 score:57.0 last_20:85.95\n",
      "episode:340 score:33.0 last_20:60.5\n",
      "episode:360 score:62.0 last_20:46.8\n",
      "episode:380 score:37.0 last_20:46.6\n",
      "episode:400 score:39.0 last_20:29.4\n",
      "episode:420 score:119.0 last_20:50.75\n",
      "episode:440 score:70.0 last_20:70.2\n",
      "episode:460 score:76.0 last_20:77.65\n",
      "episode:480 score:54.0 last_20:61.5\n",
      "episode:500 score:55.0 last_20:64.9\n"
     ]
    }
   ],
   "source": [
    "episode_rewards = []\n",
    "lr=0.01\n",
    "for i in range(nepisode+1):\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "\n",
    "\n",
    "    while not done: \n",
    "        probs_np=actor_np.forward(state)\n",
    "        ath_np=actor_np.h.copy()\n",
    "        a= np.random.choice([0,1],p=probs_np[0])   \n",
    "        next_state, reward, done, info = env.step(a)\n",
    "        total_reward += reward   \n",
    "        \n",
    "        y_next=critic_np.forward(next_state.reshape(1,-1) )\n",
    "        th_next=critic_np.h.copy() \n",
    "        y=critic_np.forward(state.reshape(1,-1))\n",
    "        th=critic_np.h.copy() \n",
    "        yhat=reward + (1-done)*gamma*y_next\n",
    "        \n",
    "        \n",
    "        grads_critic_np=grads_manual_critic_np(yhat, y, th, th_next, state, next_state, critic_np.model['w2'], done, N=1)\n",
    "        \n",
    "        for k,v in grads_critic_np.items():\n",
    "            critic_np.model[k] -=lr*grads_critic_np[k]\n",
    "        \n",
    "        advantage = yhat - y\n",
    "        yt=np.eye(2)[a].reshape(1,-1)  \n",
    "        grads_actor_np=grads_manual_np(probs_np, yt, ath_np, state, actor_np.model['w2'], advantage , N=1)\n",
    "\n",
    "        for k,v in grads_actor_np.items():\n",
    "            actor_np.model[k] -=lr*grads_actor_np[k]\n",
    "        \n",
    "        \n",
    "        state = next_state\n",
    "            \n",
    "    episode_rewards.append(total_reward)\n",
    "    if i%20==0:\n",
    "        avg20=np.mean(episode_rewards[-20:])\n",
    "        print(f'episode:{i} score:{total_reward} last_20:{avg20}')\n",
    "        if avg20>=env.spec.reward_threshold:\n",
    "            print(f'-------solved in {i} steps-------')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4bacdfc2-ee05-4974-ba50-7221e7ca7dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_r 75.35\n"
     ]
    }
   ],
   "source": [
    "avg_r=evaluate_model_np(env, actor_np)\n",
    "print('avg_r',avg_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b533724-9f40-4c49-9409-e545fc24b6a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa74dac-9002-4321-8ef8-f5100e491d42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b27149-43d0-47cb-b5b1-1908da709681",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8bb4b3f-f663-4173-98cf-65ed41254266",
   "metadata": {},
   "source": [
    "### torch and np grad check starts (worked good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f95dee41-fa29-4cdf-b22b-09c6421bb836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_check(grad_auto, grad_manual):\n",
    "    grad_auto=grad_auto.ravel()\n",
    "    grad_manual=grad_manual.ravel()\n",
    "    \n",
    "    diff=(grad_auto -  grad_manual)\n",
    "    \n",
    "    er=np.linalg.norm(diff) / ( np.linalg.norm(grad_auto)+np.linalg.norm(grad_manual) )\n",
    "    \n",
    "    return er"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cb38ecbe-73c0-4e07-8613-27e26c638b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor=NN_Open_Torch(state_dim, 128, n_actions)\n",
    "critic=NN_Open_Torch(state_dim, 128, 1) \n",
    "\n",
    "with torch.no_grad():\n",
    "    actor.tW1.data=sw1.clone()\n",
    "    actor.tb1.data=sb1.clone()\n",
    "    actor.tW2.data=sw2.clone()\n",
    "    actor.tb2.data=sb2.clone() \n",
    "    \n",
    "    critic.tW1.data=sw1c.clone()\n",
    "    critic.tb1.data=sb1c.clone()\n",
    "    critic.tW2.data=sw2c.clone()\n",
    "    critic.tb2.data=sb2c.clone() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2a8f110e-5c87-4944-8cfe-16c499d3be93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple two layer neural network\n",
      "creating nn: #input:4 #hidden:128 #output:2\n",
      "simple two layer neural network\n",
      "creating nn: #input:4 #hidden:128 #output:1\n"
     ]
    }
   ],
   "source": [
    "actor_np=TwoNN(state_dim, 128, n_actions)\n",
    "critic_np=TwoNN(state_dim, 128, 1) \n",
    "\n",
    "with torch.no_grad():\n",
    "    actor_np.model['w1']=sw1.numpy().copy()\n",
    "    actor_np.model['b1']=sb1.numpy().copy()\n",
    "    actor_np.model['w2']=sw2.numpy().copy()\n",
    "    actor_np.model['b2']=sb2.numpy().copy()\n",
    "    \n",
    "    critic_np.model['w1']=sw1c.numpy().copy()\n",
    "    critic_np.model['b1']=sb1c.numpy().copy()\n",
    "    critic_np.model['w2']=sw2c.numpy().copy()\n",
    "    critic_np.model['b2']=sb2c.numpy().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "985d9713-32c8-4ea8-bcd7-c841c46083dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "state=np.array([0.21576258, 0.6811431 , 0.22137272, 0.42765245])\n",
    "next_state=np.array([0.21576258, 0.6811431 , 0.22137272, 0.42765245])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ab61efe2-0be9-4be4-982e-394d6f12270e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ath, probs=actor.forward(t(state)) \n",
    "action= torch.argmax(probs)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c74c671a-164b-423c-8d9a-011515dd260f",
   "metadata": {},
   "outputs": [],
   "source": [
    "th_next, y_next=critic.forward(t(next_state))\n",
    "yhat=reward + (1-done)*gamma*y_next\n",
    "th, y=critic.forward(t(state)) \n",
    "advantage = yhat - y\n",
    "gradsm_critic=grads_manual_critic(yhat, y, th, th_next, state, next_state, critic.tW2, done)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "db885a80-c318-420e-9aa0-bfa74c07e2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_44320/3130668824.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dz1=torch.tensor(dh)\n"
     ]
    }
   ],
   "source": [
    "advantage = yhat - y\n",
    "yt=np.eye(2)[action]\n",
    "yt=yt.reshape(1,-1)\n",
    "gradsm_actor=grads_manual(probs.detach(),  t(yt),  ath, state.reshape(1,-1), actor.tW2, advantage)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaf622a-5711-4dfb-847f-c9d50348cbe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9e537b84-7dd3-48a3-a041-568a3fb42cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_np=actor_np.forward(state.reshape(1,-1)) \n",
    "action_np = np.argmax(probs_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "232fd922-309c-41d8-b31f-bb33cc83e1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_next=critic_np.forward(next_state.reshape(1,-1) )\n",
    "th_next=critic_np.h.copy()\n",
    "y=critic_np.forward(state.reshape(1,-1))\n",
    "th=critic_np.h.copy()\n",
    "yhat=reward + (1-done)*gamma*y_next\n",
    "advantage_np = yhat - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fbdfc59a-bb15-4c85-b714-d1b5f2495276",
   "metadata": {},
   "outputs": [],
   "source": [
    "grads_np=grads_manual_np(probs_np, yt, actor_np.h, state, actor_np.model['w2'], advantage_np, N=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6cdb5834-d250-4821-8948-58c61a34a49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "grads_critic_np=grads_manual_critic_np(yhat, y, th, th_next, state, next_state, critic_np.model['w2'], done, N=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3007b93a-c460-4b63-af5c-ef9b3bddf6dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f511d0a5-66c7-4d37-9116-dd5517da0e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.756631261872353e-08"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "er=grad_check(gradsm_critic['w2'].detach(), grads_critic_np['w2'])\n",
    "er"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "16483266-309a-45fc-a127-407c0196496f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.052705008686076e-08"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "er=grad_check(gradsm_critic['b2'].detach(), grads_critic_np['b2'])\n",
    "er"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7ce552c1-5563-4b78-8261-cd04b2d792ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.88389814312544e-08"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "er=grad_check(gradsm_critic['w1'].detach(), grads_critic_np['w1'])\n",
    "er"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2a291279-5916-4050-bdfd-eb3114c40117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.283015673474458e-08"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "er=grad_check(gradsm_critic['b1'].detach(), grads_critic_np['b1'])\n",
    "er"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db130a1c-bf4c-4d34-a87a-a0d8dde938d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69910cb5-4007-4186-ae84-c8586c995b1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f52c2791-96b9-46bf-9fde-eedc03f242b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.033124909571885e-08"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "er=grad_check(gradsm_actor['w2'].detach(), grads_np['w2'])\n",
    "er"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9cc248f0-e6f8-4935-bfae-d8641a461529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.277702405605708e-08"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "er=grad_check(gradsm_actor['b2'].detach(), grads_np['b2'])\n",
    "er"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7682ca29-ab58-49d3-9b14-53e0d9d186a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.2177150716985e-08"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "er=grad_check(gradsm_actor['w1'].detach(), grads_np['w1'])\n",
    "er"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7a512b89-5eb1-43a4-9b31-63e0177ef62f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.21771484481724e-08"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "er=grad_check(gradsm_actor['b1'].detach(), grads_np['b1'])\n",
    "er"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8df3d8-921f-4d6d-ae3b-a4aff0e24cbf",
   "metadata": {},
   "source": [
    "### torch and np grad check end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d396e5-d7e3-4ce7-b1af-5ab0ab2fc324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6ad0dc-45fa-46d8-a79f-9b35662eb1ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f9ece1-7ee4-4956-98d0-21eae2f2232a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ada28e7-b247-46e6-9597-27b462ab4d4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d6d568-163e-4604-a533-7f4ea06a4830",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
