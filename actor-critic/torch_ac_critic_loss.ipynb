{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3648b7f-1c38-492e-8c6e-896943e60abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c07170-b361-4592-a880-50313747d09f",
   "metadata": {},
   "source": [
    "## challange solved in 3 weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac096c67-23d8-43fe-b5ab-c601b71d2b82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "741a815f-cdce-446b-b3df-2d3b0e6530eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name='CartPole-v1'\n",
    "env_name='CartPole-v0'\n",
    "env = gym.make(env_name)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59cf465e-0187-478c-88a2-ea9a50873842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input features:  4  output class:  2\n"
     ]
    }
   ],
   "source": [
    "seed=4\n",
    "np.random.seed(seed)\n",
    "env.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "N,D=1, 4\n",
    "H= 128     \n",
    "C=2 \n",
    "\n",
    "sw1 = 0.04*torch.randn(H , D,  dtype=torch.float) \n",
    "sb1 = torch.zeros((1,H))\n",
    "sw2 = 0.04*torch.randn(C, H, dtype=torch.float) \n",
    "sb2 = torch.zeros((1,C))\n",
    "\n",
    "sw1c = torch.randn(H , D,  dtype=torch.float)   #for critic\n",
    "sb1c = torch.zeros((1,H))                           #for critic\n",
    "sw2c = torch.randn(1, H, dtype=torch.float)    #for critic\n",
    "sb2c = torch.zeros((1,1))                           #for critic\n",
    "\n",
    "\n",
    "print('input features: ',D ,' output class: ',C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "275aba82-9fbb-4ea1-8fed-d7a640c8e0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def t(x): return torch.from_numpy(x).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2be5b8cf-17bf-4382-a33a-f34d139d89e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    #same performance with torch.optim.Adam\n",
    "    def __init__(self, model_params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8):\n",
    "        self.params = list(model_params)\n",
    "        self.lr = lr\n",
    "        self.beta_1, self.beta_2 = betas\n",
    "        self.eps = eps\n",
    "        self.M= [torch.zeros_like(p) for p in self.params]\n",
    "        self.V = [torch.zeros_like(p) for p in self.params]\n",
    "        self.n_steps = 0\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            param.grad = None\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        self.n_steps += 1 \n",
    "        for i in range(len(self.params)): \n",
    "            dw=self.params[i].grad\n",
    "             \n",
    "            self.M[i]= self.M[i]*self.beta_1 + (1-self.beta_1) * dw\n",
    "            self.V[i] = self.V[i] *self.beta_2 + (1 - self.beta_2)* dw**2\n",
    " \n",
    "            m_hat = self.M[i] / (1 - self.beta_1 ** self.n_steps)  #bias correction\n",
    "            v_hat = self.V[i] / (1 - self.beta_2 ** self.n_steps) #bias correction\n",
    "             \n",
    "            self.params[i] -= self.lr * m_hat / (torch.sqrt(v_hat)+self.eps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdbc84b5-bf6c-48d1-bd63-79aa17b6712d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_Open_Torch:\n",
    "    def __init__(self, D, H, C):\n",
    "        self.tW1 = torch.randn(H , D,  dtype=torch.float, requires_grad=True)\n",
    "        self.tb1 = torch.zeros((1,H), requires_grad=True)\n",
    "        self.tW2 = torch.randn(C,H, dtype=torch.float, requires_grad=True)\n",
    "        self.tb2 = torch.zeros((1,C), requires_grad=True)\n",
    "        self.C=C\n",
    "        self.params=[self.tW1, self.tb1, self.tW2, self.tb2]\n",
    "        self.ws={'w1':self.tW1, 'b1':self.tb1, 'w2':self.tW2, 'b2':self.tb2}\n",
    "        \n",
    "    def parameters(self):\n",
    "        return self.params \n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            param.grad=None\n",
    "        \n",
    "    def forward(self, X):\n",
    "        th= torch.relu( X   @ self.tW1.T  + self.tb1) \n",
    "        z=th @self.tW2.T  + self.tb2 \n",
    "        if self.C==1:\n",
    "            return th, z\n",
    "        \n",
    "        tP=torch.softmax( z , axis=1)\n",
    "        return th, tP\n",
    "    \n",
    "    def grads_manual(self, tP, y_train, th, X):\n",
    "        grads={}\n",
    "\n",
    "        dz2=(tP-y_train)  \n",
    "        dW2=(dz2.T  @  th)/N  \n",
    "        dh=(dz2 @  self.tW2  )/N\n",
    "        db2=torch.sum(dz2 , axis=0 ) /N\n",
    "\n",
    "\n",
    "        dz1=torch.tensor(dh)\n",
    "        dz1[th<=0]=0 \n",
    "\n",
    "        dW1= dz1.T @ X \n",
    "        db1=torch.sum(dz1, axis=0 )\n",
    "\n",
    "        grads={'w1':dW1, 'b1':db1, 'w2':dW2, 'b2':db2}\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa98fa3b-f968-46a2-a7ed-d685c6d9fabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grads_auto(model):\n",
    "    grads={}\n",
    "    with torch.no_grad():\n",
    "        grads['w1']=model.tW1.grad.clone()\n",
    "        grads['b1']=model.tb1.grad.clone()\n",
    "        grads['w2']=model.tW2.grad.clone()\n",
    "        grads['b2']=model.tb2.grad.clone()\n",
    "        \n",
    "        model.tb1.grad.zero_() \n",
    "        model.tW1.grad.zero_() \n",
    "        model.tb2.grad.zero_() \n",
    "        model.tW2.grad.zero_() \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9fccd14-4304-41f5-b440-b5d877ca8a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(env, num_ep=20):\n",
    "    \"\"\"\n",
    "    run env num_ep times and return average reward.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for ep in range(num_ep):\n",
    "        state = env.reset() \n",
    "        ep_reward=0\n",
    "        for tt in range(2000):\n",
    "            h,probs = actor.forward(t(state))\n",
    "            dist = torch.distributions.Categorical(probs=probs[0])\n",
    "            action = dist.sample()\n",
    "            action=action.detach().numpy() \n",
    "\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            ep_reward+=reward\n",
    "            if done:\n",
    "                # print(f\"ep:{ep} reward:{ep_reward}\")\n",
    "                break \n",
    "        \n",
    "        rewards.append(ep_reward)\n",
    "    return np.array(rewards).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98017a4-3488-4f32-985f-3f2c55334111",
   "metadata": {},
   "source": [
    "### run1: base 59.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa50291a-7b13-4f0b-ad65-a84cf46d1027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fcb68032470>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "env.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "857d0a4b-e635-4a11-8dc8-a029dda89e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor=NN_Open_Torch(state_dim, 128, n_actions)\n",
    "critic=NN_Open_Torch(state_dim, 128, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac97685f-4e13-49fd-8216-015331e8a645",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    actor.tW1.data=sw1.clone()\n",
    "    actor.tb1.data=sb1.clone()\n",
    "    actor.tW2.data=sw2.clone()\n",
    "    actor.tb2.data=sb2.clone() \n",
    "    \n",
    "    critic.tW1.data=sw1c.clone()\n",
    "    critic.tb1.data=sb1c.clone()\n",
    "    critic.tW2.data=sw2c.clone()\n",
    "    critic.tb2.data=sb2c.clone() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7383cd2-1f58-448d-84dd-37e6dbf4c66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52595bdf-eb6b-41de-9497-dc0670ba523e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:0 score:15.0 last_20:15.0\n",
      "episode:20 score:15.0 last_20:14.45\n",
      "episode:40 score:15.0 last_20:19.1\n",
      "episode:60 score:62.0 last_20:32.25\n",
      "episode:80 score:24.0 last_20:41.75\n",
      "episode:100 score:14.0 last_20:48.6\n",
      "episode:120 score:21.0 last_20:62.35\n",
      "episode:140 score:19.0 last_20:21.45\n",
      "episode:160 score:29.0 last_20:23.7\n",
      "episode:180 score:44.0 last_20:35.1\n",
      "episode:200 score:50.0 last_20:57.2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "episode_rewards = []\n",
    "lr=0.01\n",
    "for i in range(200+1):\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "\n",
    "\n",
    "    while not done:\n",
    "        x=t(state ) \n",
    "        th, probs=actor.forward(x) \n",
    "        dist = torch.distributions.Categorical(probs=probs[0])\n",
    "        action = dist.sample()\n",
    "        \n",
    "        a=action.detach().data.numpy()  \n",
    "        next_state, reward, done, info = env.step(a)\n",
    "        advantage = reward + (1-done)*gamma*critic.forward(t(next_state))[1] - critic.forward(t(state))[1]\n",
    "        \n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "\n",
    "        critic_loss = 0.5*advantage.pow(2).mean() \n",
    "        critic.zero_grad()\n",
    "        critic_loss.backward() \n",
    "        # adam_critic.step() \n",
    "        with torch.no_grad():\n",
    "            for param in critic.parameters():\n",
    "                param.data -=lr*param.grad \n",
    "        \n",
    "        \n",
    "         \n",
    "        actor_loss = -dist.log_prob(action)*advantage.detach()\n",
    "        actor.zero_grad() \n",
    "        actor_loss.backward()\n",
    "        # adam_actor.step() \n",
    "        \n",
    "        with torch.no_grad(): \n",
    "            for param in actor.parameters():\n",
    "                param.data -=lr*param.grad\n",
    "            \n",
    "    episode_rewards.append(total_reward)\n",
    "    if i%20==0:\n",
    "        avg20=np.mean(episode_rewards[-20:])\n",
    "        print(f'episode:{i} score:{total_reward} last_20:{avg20}')\n",
    "        if avg20>=env.spec.reward_threshold:\n",
    "            print(f'-------solved in {i} steps-------')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27d20faf-3559-44b9-be60-895d91876ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_r 68.0\n"
     ]
    }
   ],
   "source": [
    "avg_r=evaluate_model(env)\n",
    "print('avg_r',avg_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95beb130-ef2b-45c7-bca6-74759230594a",
   "metadata": {},
   "source": [
    "### end of run1\n",
    "\n",
    "### now run2: using auto_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2795176a-0b83-45bd-ab9b-8548694b72ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "env.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "actor=NN_Open_Torch(state_dim, 128, n_actions)\n",
    "critic=NN_Open_Torch(state_dim, 128, 1) \n",
    "\n",
    "with torch.no_grad():\n",
    "    actor.tW1.data=sw1.clone()\n",
    "    actor.tb1.data=sb1.clone()\n",
    "    actor.tW2.data=sw2.clone()\n",
    "    actor.tb2.data=sb2.clone() \n",
    "    \n",
    "    critic.tW1.data=sw1c.clone()\n",
    "    critic.tb1.data=sb1c.clone()\n",
    "    critic.tW2.data=sw2c.clone()\n",
    "    critic.tb2.data=sb2c.clone() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f96ab3d-c125-4e8c-b358-bad71b370dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9c862b5-7518-447f-b059-6e5b811c904e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:0 score:15.0 last_20:15.0\n",
      "episode:20 score:15.0 last_20:14.45\n",
      "episode:40 score:15.0 last_20:19.1\n",
      "episode:60 score:62.0 last_20:32.25\n",
      "episode:80 score:24.0 last_20:41.75\n",
      "episode:100 score:14.0 last_20:48.6\n",
      "episode:120 score:21.0 last_20:62.35\n",
      "episode:140 score:19.0 last_20:21.45\n",
      "episode:160 score:29.0 last_20:23.7\n",
      "episode:180 score:44.0 last_20:35.1\n",
      "episode:200 score:50.0 last_20:57.2\n"
     ]
    }
   ],
   "source": [
    "episode_rewards = []\n",
    "lr=0.01\n",
    "for i in range(200+1):\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "\n",
    "\n",
    "    while not done:\n",
    "        x=t(state ) \n",
    "        th, probs=actor.forward(x) \n",
    "        dist = torch.distributions.Categorical(probs=probs[0])\n",
    "        action = dist.sample()\n",
    "        \n",
    "        a=action.detach().data.numpy()  \n",
    "        next_state, reward, done, info = env.step(a)\n",
    "        yhat=reward + (1-done)*gamma*critic.forward(t(next_state))[1]\n",
    "        y=critic.forward(t(state))[1]\n",
    "        advantage = yhat - y\n",
    "        \n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "\n",
    "        critic_loss = 0.5*advantage.pow(2).mean() \n",
    "        critic.zero_grad()\n",
    "        critic_loss.backward() \n",
    "        # gradsm_critic=grads_manual(yhat,  y, th, state.reshape(1,-1), critic.tW2)   \n",
    "        gradsm_critic=grads_auto(critic)\n",
    "        \n",
    "        with torch.no_grad(): \n",
    "            for k,v in gradsm_critic.items():\n",
    "                critic.ws[k] -=lr*gradsm_critic[k]\n",
    " \n",
    "        actor_loss = -dist.log_prob(action)*advantage.detach()\n",
    "        actor.zero_grad() \n",
    "        actor_loss.backward()\n",
    "        gradsm_actor=grads_auto(actor)\n",
    "        \n",
    "        with torch.no_grad(): \n",
    "            for k,v in gradsm_actor.items():\n",
    "                actor.ws[k] -=lr*gradsm_actor[k]\n",
    "                \n",
    "            \n",
    "    episode_rewards.append(total_reward)\n",
    "    if i%20==0:\n",
    "        avg20=np.mean(episode_rewards[-20:])\n",
    "        print(f'episode:{i} score:{total_reward} last_20:{avg20}')\n",
    "        if avg20>=env.spec.reward_threshold:\n",
    "            print(f'-------solved in {i} steps-------')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3f27935-d514-4754-98c6-114617ef87db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_r 68.0\n"
     ]
    }
   ],
   "source": [
    "avg_r=evaluate_model(env)\n",
    "print('avg_r',avg_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afc7864-a296-476f-9842-8c956f72a7d6",
   "metadata": {},
   "source": [
    "### end of run2\n",
    "### Now, run3: actor manual_grad, critic augo_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da611c1d-e295-4bd3-a80c-339cfc86f789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grads_manual(tP, y_train, th, X, W2, adv, N=1):\n",
    "    # print('shape:', tP.shape, y_train.shape)\n",
    "    grads={}\n",
    "    \n",
    "    dz2=(tP-y_train)*adv  /N \n",
    "    dW2=(dz2.T  @  th) \n",
    "    dh=(dz2 @   W2  ) \n",
    "    db2=torch.sum(dz2 , axis=0 ) \n",
    "\n",
    "\n",
    "    dz1=torch.tensor(dh)   \n",
    "    dz1[th<=0]=0                     #equal sign is extremely important. \n",
    "\n",
    "    dW1=( dz1.T @ X) \n",
    "    db1=torch.sum(dz1, axis=0 ) \n",
    "    \n",
    "    grads={'w1':dW1, 'b1':db1, 'w2':dW2, 'b2':db2}\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff88c9f2-fa05-4c27-965b-1ed0511c0bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "env.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "actor=NN_Open_Torch(state_dim, 128, n_actions)\n",
    "critic=NN_Open_Torch(state_dim, 128, 1) \n",
    "\n",
    "with torch.no_grad():\n",
    "    actor.tW1.data=sw1.clone()\n",
    "    actor.tb1.data=sb1.clone()\n",
    "    actor.tW2.data=sw2.clone()\n",
    "    actor.tb2.data=sb2.clone() \n",
    "    \n",
    "    critic.tW1.data=sw1c.clone()\n",
    "    critic.tb1.data=sb1c.clone()\n",
    "    critic.tW2.data=sw2c.clone()\n",
    "    critic.tb2.data=sb2c.clone() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bfd8de1c-7983-4505-8223-c9cbcd4aead4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:0 score:15.0 last_20:15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8765/3130668824.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dz1=torch.tensor(dh)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:20 score:15.0 last_20:14.45\n",
      "episode:40 score:15.0 last_20:19.1\n",
      "episode:60 score:62.0 last_20:32.25\n",
      "episode:80 score:24.0 last_20:41.75\n",
      "episode:100 score:14.0 last_20:48.6\n",
      "episode:120 score:21.0 last_20:62.35\n",
      "episode:140 score:19.0 last_20:21.45\n",
      "episode:160 score:29.0 last_20:23.7\n",
      "episode:180 score:44.0 last_20:35.1\n",
      "episode:200 score:50.0 last_20:57.2\n"
     ]
    }
   ],
   "source": [
    "episode_rewards = []\n",
    "lr=0.01\n",
    "for i in range(200+1):\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "\n",
    "\n",
    "    while not done:\n",
    "        x=t(state ) \n",
    "        ath, probs=actor.forward(x) \n",
    "        dist = torch.distributions.Categorical(probs=probs[0])\n",
    "        action = dist.sample()\n",
    "        \n",
    "        a=action.detach().data.numpy()  \n",
    "        next_state, reward, done, info = env.step(a)\n",
    "        yhat=reward + (1-done)*gamma*critic.forward(t(next_state))[1]\n",
    "        y=critic.forward(t(state))[1]\n",
    "        advantage = yhat - y\n",
    "        \n",
    "        total_reward += reward \n",
    "\n",
    "        critic_loss = 0.5*advantage.pow(2).mean() \n",
    "        critic.zero_grad()\n",
    "        critic_loss.backward()    \n",
    "        gradsm_critic=grads_auto(critic)\n",
    "        \n",
    "        with torch.no_grad(): \n",
    "            for k,v in gradsm_critic.items():\n",
    "                critic.ws[k] -=lr*gradsm_critic[k]\n",
    " \n",
    "        actor_loss = -dist.log_prob(action)*advantage.detach()\n",
    "        actor.zero_grad() \n",
    "        actor_loss.backward()\n",
    "        gradsm_actor_auto=grads_auto(actor)\n",
    "        yt=np.eye(2)[action]\n",
    "        yt=yt.reshape(1,-1)\n",
    "        gradsm_actor=grads_manual(probs.detach(),  t(yt),  ath, state.reshape(1,-1), actor.tW2, advantage)   \n",
    "        \n",
    "        with torch.no_grad(): \n",
    "            for k,v in gradsm_actor.items():\n",
    "                actor.ws[k] -=lr*gradsm_actor[k]\n",
    "        \n",
    "        state = next_state\n",
    "            \n",
    "    episode_rewards.append(total_reward)\n",
    "    if i%20==0:\n",
    "        avg20=np.mean(episode_rewards[-20:])\n",
    "        print(f'episode:{i} score:{total_reward} last_20:{avg20}')\n",
    "        if avg20>=env.spec.reward_threshold:\n",
    "            print(f'-------solved in {i} steps-------')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d18114c4-ed52-43c9-b7fe-4e096ee5aadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_r 68.0\n"
     ]
    }
   ],
   "source": [
    "avg_r=evaluate_model(env)\n",
    "print('avg_r',avg_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d309f8d-eac4-43c6-a500-39d1a778c956",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73740c02-5c94-48ba-a083-21570a98a692",
   "metadata": {},
   "source": [
    "### end of run3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7fb609-d7ba-46cf-9ac1-2b3bcb812d27",
   "metadata": {},
   "source": [
    "### run 4: actor, critic both manual grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ba7fee5-ad35-498b-8894-28a0f9f2fded",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "env.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "actor=NN_Open_Torch(state_dim, 128, n_actions)\n",
    "critic=NN_Open_Torch(state_dim, 128, 1) \n",
    "\n",
    "with torch.no_grad():\n",
    "    actor.tW1.data=sw1.clone()\n",
    "    actor.tb1.data=sb1.clone()\n",
    "    actor.tW2.data=sw2.clone()\n",
    "    actor.tb2.data=sb2.clone() \n",
    "    \n",
    "    critic.tW1.data=sw1c.clone()\n",
    "    critic.tb1.data=sb1c.clone()\n",
    "    critic.tW2.data=sw2c.clone()\n",
    "    critic.tb2.data=sb2c.clone() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "384714c4-5250-4d28-88bd-e48126a24977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grads_manual_critic(yhat, y, th, th_next, state, next_state, w2, done, N=1):\n",
    "    grads={}\n",
    "    \n",
    "    dz2=(yhat - y)  /N \n",
    "    cth=(1-done)*gamma*th_next-th\n",
    "    dw2=(dz2.T  @  cth) \n",
    "\n",
    "    dh=(dz2 @   w2  ) \n",
    "\n",
    "    db2=torch.sum(dz2*((1-done)*gamma-1) , axis=0 ) \n",
    "\n",
    "    \n",
    "    # dz1n=torch.tensor(dh)\n",
    "    dz1n=dh.detach().clone()\n",
    "    dz1n[th_next<=0]=0                     #equal sign is extremely important. \n",
    "    \n",
    "    \n",
    "    # dz1=torch.tensor(dh) \n",
    "    dz1=dh.detach().clone()\n",
    "    dz1[th<=0]=0                     #equal sign is extremely important. \n",
    "\n",
    "     \n",
    "    # dw1=(1-done)*(gamma* ( dz1n.T @ t(next_state.reshape(1,-1)) ) - dz1.T @ t(state.reshape(1,-1) ) )\n",
    "    dw1=(1-done)* gamma* dz1n.T @ t(next_state.reshape(1,-1)) - dz1.T @ t(state.reshape(1,-1) ) \n",
    "    \n",
    "    \n",
    "    db1=torch.sum( ((1-done)*gamma*dz1n -dz1), axis=0 )\n",
    "    \n",
    "    grads={'w1':dw1, 'b1':db1, 'w2':dw2, 'b2':db2}\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7593314b-57bd-47b4-b39d-fbb0ecdd5d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:0 score:15.0 last_20:15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8765/3130668824.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dz1=torch.tensor(dh)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:20 score:15.0 last_20:14.45\n",
      "episode:40 score:15.0 last_20:19.1\n",
      "episode:60 score:62.0 last_20:32.25\n",
      "episode:80 score:24.0 last_20:41.75\n",
      "episode:100 score:14.0 last_20:48.6\n",
      "episode:120 score:21.0 last_20:62.35\n",
      "episode:140 score:19.0 last_20:21.45\n",
      "episode:160 score:29.0 last_20:23.7\n",
      "episode:180 score:44.0 last_20:35.1\n",
      "episode:200 score:50.0 last_20:57.2\n"
     ]
    }
   ],
   "source": [
    "episode_rewards = []\n",
    "lr=0.01\n",
    "for i in range(200+1):\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "\n",
    "\n",
    "    while not done:\n",
    "        x=t(state ) \n",
    "        ath, probs=actor.forward(x) \n",
    "        dist = torch.distributions.Categorical(probs=probs[0])\n",
    "        action = dist.sample()\n",
    "        \n",
    "        a=action.detach().data.numpy()  \n",
    "        next_state, reward, done, info = env.step(a)\n",
    "        \n",
    "        th_next, y_next=critic.forward(t(next_state))\n",
    "        yhat=reward + (1-done)*gamma*y_next\n",
    "        th, y=critic.forward(t(state)) \n",
    "        advantage = yhat - y\n",
    "       \n",
    "        total_reward += reward \n",
    "\n",
    "        critic_loss = 0.5*advantage.pow(2).mean()  \n",
    "        gradsm_critic=grads_manual_critic(yhat, y, th, th_next, state, next_state, critic.tW2, done)\n",
    "        \n",
    "        with torch.no_grad(): \n",
    "            for k,v in gradsm_critic.items():\n",
    "                critic.ws[k] -=lr*gradsm_critic[k]\n",
    " \n",
    "        actor_loss = -dist.log_prob(action)*advantage.detach() \n",
    "        yt=np.eye(2)[action]\n",
    "        yt=yt.reshape(1,-1)\n",
    "        gradsm_actor=grads_manual(probs.detach(),  t(yt),  ath, state.reshape(1,-1), actor.tW2, advantage)   \n",
    "        \n",
    "        with torch.no_grad(): \n",
    "            for k,v in gradsm_actor.items():\n",
    "                actor.ws[k] -=lr*gradsm_actor[k]\n",
    "        \n",
    "        state = next_state\n",
    "            \n",
    "    episode_rewards.append(total_reward)\n",
    "    if i%20==0:\n",
    "        avg20=np.mean(episode_rewards[-20:])\n",
    "        print(f'episode:{i} score:{total_reward} last_20:{avg20}')\n",
    "        if avg20>=env.spec.reward_threshold:\n",
    "            print(f'-------solved in {i} steps-------')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f3763964-ca41-47ed-a5b5-935281e40a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_r 68.0\n"
     ]
    }
   ],
   "source": [
    "avg_r=evaluate_model(env)\n",
    "print('avg_r',avg_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c68edcb-5b5d-4b0c-b93f-f3e899483f82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57055fd7-155f-485b-bd09-5f6ecfaa7dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbccf44-4ce7-4561-afc4-877057e0ac53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95dee41-fa29-4cdf-b22b-09c6421bb836",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed1623d-625e-41c6-80f6-42b113149dd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919c632c-d9c9-4735-bec9-8581d28eab12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbd459f-c708-4c0f-baa0-f7ac61e9093d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d6d568-163e-4604-a533-7f4ea06a4830",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
