{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64ad282c-a818-419d-8c3c-f8f33c0a7a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5f26482-4ff5-4227-ba76-b4a4a63a9f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n"
     ]
    }
   ],
   "source": [
    "env_id='FrozenLake-v1'\n",
    "env_id='FrozenLake8x8-v1'\n",
    "env= gym.make(env_id)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf15f1f-1de2-4dbd-b42c-85993b7ae3d7",
   "metadata": {},
   "source": [
    "### policy iteration\n",
    "1. choose a random policy\n",
    "2. evaluate this policy by calculating state value V(s) using Bellman expectation equation\n",
    "3. using V, extract best policy as new_policy\n",
    "4. Stop when policy doesn't change ( new_policy=old_policy )\n",
    "\n",
    "$$\n",
    "V_\\pi(s) = \\sum_a \\pi(a|s) \\sum_{s',a}  p(s'|s,a)(r + \\gamma*V(s')\n",
    "$$\n",
    "\n",
    "### eqs\n",
    "\n",
    "$$ q(s,a) = \\sum_{s',a}  p(s'|s,a) \\big[r + \\gamma*V(s') \\big]$$\n",
    "\n",
    "$$V^*(s)=\\underset{a}{max} q(s,a) $$\n",
    "\n",
    "$$V_\\pi(s)=\\sum_a \\pi(a|s) q(s,a) $$\n",
    "\n",
    "$$\\pi^*(a|s)=\\underset{a}{argmax} \\space q(s,a) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a53dfdf-04cc-48c6-a786-78c59484c249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_qvalues(env, state, V, gamma):\n",
    "    # calculate state-action values q(s,a) for all the actions.\n",
    "    action_values = np.zeros(env.nA)\n",
    "    for a in range(env.nA):\n",
    "        transitions=env.P[state][a]\n",
    "        expected_actionvalue=0\n",
    "        for transition in transitions:\n",
    "            psas, next_state, reward, done=transition\n",
    "            expected_actionvalue += psas* ( reward + gamma * V[next_state] )\n",
    "\n",
    "        action_values[a] =expected_actionvalue\n",
    "    return action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aea52a43-c648-45e1-aba4-f2a02979b466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, policy, gamma, theta=1e-9, max_iterations=1e9, verbose=False):\n",
    "    #calculate state-value V_pi(s) using bellman expectation equation for all s.\n",
    "    V=np.zeros(env.nS)  \n",
    "    for i in range(int(max_iterations)):\n",
    "        max_delta=0  \n",
    "        for state in range(env.nS): \n",
    "            qs=calc_qvalues(env, state, V, gamma) \n",
    "            v=np.sum(policy[state]*qs)\n",
    "            max_delta= max(max_delta, np.abs(V[state]-v))\n",
    "            V[state]=v\n",
    "\n",
    "        if max_delta<theta:\n",
    "            print(f'converged at {i}th iteration')\n",
    "            break\n",
    "        elif verbose and  i%100==0:\n",
    "            print(f'i={i} delta={max_delta}')\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424b96dc-5a23-435e-9c2b-dbdcb0601d9e",
   "metadata": {},
   "source": [
    "### policy iteration algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db76a68f-90a5-4856-8609-bab7c86247f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7160c55-5197-4a37-b896-5dcca77737ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged at 201th iteration\n",
      "i=0\n",
      "converged at 860th iteration\n",
      "converged at 905th iteration\n",
      "converged at 956th iteration\n",
      "converged at 1063th iteration\n",
      "converged at 934th iteration\n",
      "policy got stable at 5\n"
     ]
    }
   ],
   "source": [
    "policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "max_itr=1e9\n",
    "for i in range(int(max_itr)):\n",
    "    V=evaluate_policy(env, policy, gamma)\n",
    "    stable=True\n",
    "    for state in range(env.nS):\n",
    "        policy_action=np.argmax(policy[state]) \n",
    "        action_values=calc_qvalues(env, state, V, gamma)  \n",
    "        best_action=np.argmax(action_values)\n",
    "        if policy_action!=best_action:\n",
    "            stable=False\n",
    "        policy[state] = np.eye(env.nA)[best_action] \n",
    "\n",
    "    if stable:\n",
    "        print(f'policy got stable at {i}')\n",
    "        break\n",
    "    elif i%100==0:\n",
    "            print(f'i={i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9274fa6c-4965-4701-b1d1-1cf6b3adc27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episodes(policy, episodes=1000):\n",
    "    rewards=0\n",
    "    wins=0\n",
    "    for i in range(episodes):\n",
    "        state=env.reset()\n",
    "        done=False\n",
    "        while not done:\n",
    "            action=np.argmax(policy[state])\n",
    "            next_state,reward,done,info=env.step(action)\n",
    "            rewards+=reward\n",
    "            state=next_state\n",
    "            if done and reward==1.0:\n",
    "                wins+=1\n",
    "    return wins,rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "529fb696-98d4-4cb5-82a7-b6ba8f2f4214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total play 1000 total wins 881 total rewards 881.0\n",
      "success rate 88.1%\n"
     ]
    }
   ],
   "source": [
    "episodes=1000\n",
    "wins, rewards=play_episodes(policy, episodes=episodes)\n",
    "print(f'total play {episodes} total wins {wins} total rewards {rewards}')\n",
    "print(f'success rate {(wins/episodes)*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1459d7aa-8260-4c5a-85d9-b3d616bea20f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f873d38-990c-4780-8639-d7f9aee9825c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b709725-8593-4491-8f1d-d03cba14d4fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cb66e1-c89d-47db-8885-a944044c2a80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e82d0d-eb90-496c-9aa0-d5f02c3b2b30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119a232b-e012-4c8e-979f-86b28b97a8b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
